# Adversarial Attacks 

This project explores a range of **adversarial attack techniques** in both **computer vision** and **natural language processing**. It provides hands-on implementations, visual analysis, and step-by-step explanations for each method.

ğŸ’š The goal is to demonstrate the vulnerabilities of modern deep learning models and help users understand how small, often imperceptible, perturbations can cause misclassification.

---

## ğŸ©· Project Overview

The repository includes the following Jupyter notebooks, each implementing a specific adversarial method:

| ğŸ’š Notebook | ğŸ©· Description & Justification |
|-------------|-------------------------------|
| `deepfool.ipynb` | **DeepFool** computes minimal perturbations that cross decision boundaries. Included to demonstrate how imperceptibly small changes can fool even robust models. |
| `projected_gradient_descent_method.ipynb` | **PGD (Projected Gradient Descent)** is a widely used, iterative attack and benchmark for adversarial robustness. This notebook demonstrates a strong untargeted image attack. |
| `one_multi_pixel_attacks.ipynb` | **One-Pixel and Multi-Pixel Attacks** show that even single-pixel changes can drastically affect model output. These are useful for understanding boundary sensitivity. |
| `latent_masking.ipynb` | Explores internal **latent representation manipulation** rather than raw inputs. Offers insights into vulnerabilities in feature space. |
| `CLIP.ipynb` | Attacks **CLIP models** by manipulating text-image embeddings. Included due to the growing use of multimodal models in real-world systems. |
| `TextBugger.ipynb` | **TextBugger** is a powerful NLP attack that alters text minimally while fooling sentiment or classification models. Highlights risks in language applications. |
| `fast_gradient_sign_method.ipynb` | **FGSM (Fast Gradient Sign Method)** is a foundational one-step attack based on gradient sign. Included as a baseline and educational reference. |

---

## ğŸ“Š Attack Comparison Table

This table provides a high-level comparison of all the adversarial attacks implemented in this project.

|  **Attack** |  **Type** | **Domain** |**Method** |**Access** | **Class** | **Steps** |**Perceptibility** | **Notes** |
|---------------|-------------|---------------|--------------------------|----------------|----------------|----------------|----------------------|---------------------------|
| **FGSM** | Untargeted | Vision | Gradient-based | White-box | Evasion | 1 | Low | Fast & simple baseline attack |
| **PGD** | Untargeted | Vision | Gradient-based (iterative) | White-box | Evasion | Many | Low | Stronger than FGSM; widely used benchmark |
| **DeepFool** | Untargeted | Vision | Optimization-based | White-box | Evasion | Iterative | Very Low | Computes minimal boundary-crossing perturbations |
| **One Pixel Attack** | Untargeted | Vision | Score-based | Black-box | Evasion | Few | Low | Alters only 1â€“5 pixels using optimization |
| **Latent Masking** | Untargeted | Vision | Optimization-based | White-box and Black-box| Evasion | Iterative | Medium | Perturbs internal (latent) representations |
| **CLIP Embedding Attack** | Untargeted | Multimodal | Optimization-based | White-box | Evasion | Iterative | Lowâ€“Medium | Disrupts CLIPâ€™s textâ€“image embedding alignment |
| **TextBugger** | Untargeted | NLP | Heuristic & Score-based | Black-box | Evasion | Varies | Low | Changes characters/words while preserving grammar and semantics |
| **Fast Gradient Sign Method (FGSM)** | Untargeted | Vision | Gradient-based | White-box | Evasion | 1 | Low | Classic one-step attack using gradient sign |
| **Clean-Label Feature Collision** | Targeted | Vision | Optimization-based | White-box | Poisoning | Iterative | Very Low | Inserts clean-looking samples into training to misclassify a specific target at test time |


## âš™ï¸ Installation Instructions

1.  Clone this repository:

```bash
git clone https://github.com/your-username/adversarial-attacks.git
cd adversarial-attacks
```

## ğŸ©· Install dependencies:

```bash
pip install -r requirements.txt
```

# ğŸš€ Usage Instructions
Open a Jupyter Notebook environment:

``` bash
jupyter notebook
```
ğŸ©·ğŸ©·ğŸ©· !!! **Better to run all in Google Colab** !!! ğŸ©·ğŸ©·ğŸ©·

Run any of the following notebooks:

* `deepfool.ipynb`
* `projected_gradient_descent_method.ipynb`
* `one_multi_pixel_attacks.ipynb`
* `latent_masking.ipynb`
* `CLIP.ipynb`
* `TextBugger.ipynb`
* `fast_gradient_sign_method.ipynb`
* `clean_label_feature_collision.ipynb`


Each notebook includes:

* ` Model loading`
* `Attack generation`
* `Saliency and perturbation visualizations`
* `Interpretation of results`
     

## ğŸ’» Cross-Platform Compatibility

This project works on:

    ğŸ©· Linux â€“ Fully supported. Recommended for GPU acceleration and large model training.

    ğŸ©· macOS â€“ Fully supported for CPU-based execution. GPU support is limited due to PyTorch constraints.

    ğŸ©· Windows â€“ Fully supported. Make sure to properly activate venv and use pip.

  Make sure to use a GPU-enabled environment (e.g., CUDA-compatible machine or Google Colab) for optimal performance. 

## â¤ï¸â€ğŸ©¹ Requirements Snapshot

Key libraries (see requirements.txt for full list):

    torch==2.6.0+cu124
    torchvision==0.15.2
    numpy==1.26.2
    matplotlib==3.7.1
    Pillow==9.5.0
    requests==2.31.0
    urllib3==2.0.7
    transformers==4.30.0
    diffusers==0.18.0
    torchattacks==3.5.1
    nltk==3.8.1


## â­ï¸ If You Found This Helpful...

Please â­ï¸ this repo â€” it wonâ€™t stop Duolingo from reminding you to practice Spanish, but it will help us to boost the grades (we hope so)â¤ï¸â€ğŸ©¹â¤ï¸â€ğŸ©¹â¤ï¸â€ğŸ©¹

